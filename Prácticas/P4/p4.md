`ISE` > Prácticas > **Práctica 4.** Benchmarking

## Benchmarking con Phoronix

* Benchmark en distintos lenguajes, deben compilarse para poder ejecutarse.
  * Hay que ver qué lenguajes para ver las dependencias necesarias.
* Tiene tests individuales y suites (conjunto de benchmarks que se reúnen en una colección bajo un título, se suelen reunir los benchmarks que tienen sentido para una aplicación concreta; ej.: servidor web)
  * Ejecutaremos tests individuales porque las suites son muy grandes

Lo usaremos en Ubuntu.

* La suite se instala como un paquete más de Ubuntu, lo buscamos:

  ~~~
  apt-cache search phoronix
  apt-get install phoronix-test-suite
  ~~~

* Vemos todas las opciones haciendo directamente `phoronix-test-suite`

* Información del equipo: `phoronix-test-suite system-info`

* Información de sensores: `phoronix-test-suite system-sensors`

  * En la máquina virtual muchos sensores (frecuencia de ventilador, temperatura...) no los tenemos

* Listado de las suites disponibles: no baja nada, no ocupa espacio en disco (como un `apt-cache search`):

  ~~~
  phoronix-test-suite list-available-suites
  ~~~

  * Si nos salta el error de que la información viene comprimida, es porque necesita `unzip` instalado.

* Vemos los tests individuales

  ~~~
  phoronix-test-suite list-available-tests
  ~~~

  * Los gráficos no funcionarán con la MV, necesitan entorno de ventanas.
  * Mejor probar con los de procesador y memoria

* Vemos información de un test

  ~~~
  phoronix-test-suite info pts/stream
  ~~~

  * Esto tampoco ocupa espacio en memoria, es una consulta a Phoronix
  * Vemos que tienen dependencias software: _compiler / development libraries_

* Ejecutaremos un test con pocas dependencias, como `pts/compress-gzip`

  * Para ejecutarlo tenemos dos opciones:

    ~~~
    phoronix-test-suite benchmark pts/compress-gzip
    	// se baja el test y lo ejecuta
    phoronix-test-suite install pts/compress-gzip	// bajar test
    phoronix-test-suite run pts/compress-gzip		// ejecutar test
    ~~~

* Vemos que nos da los _test results_.

* Vemos los resultados que hemos almacenado:

  ~~~
  phoronix-test-suite list-saved-results
  phoronix-test-suite result-file-to-text ise2020
  ~~~

###### Tarea

Buscar un test con una documentación significativa para ejecutarlo y analizar los resultados. No hace falta entregar nada.

## Apache Benchmark: `ab`

* Exclusivo de HTTP, simula carga de servidores HTTP para cualquier servidor de HTTP, no únicamente Apache Server.

* Se ejecuta como una sola instrucción por línea de comandos.

* Instalamos `ab`.

* ~~~
  ab -n 10 -c 5 http://www.ugr.es/
  ~~~

  * `-n` número total de peticiones GET
  * `-c` simular concurrencia (varios usuarios en paralelo), cada proceso ejecutará 2 llamadas
  * Document length tiene 162 bytes: sospechamos que no es el documento de la web
    * Hacemos `curl "http://www.ugr.es/"`
    * Vemos una página de redirección, nos dice que tenemos que hacer una llamada a HTTPS
    * Ahora hacemos `ab` a `https://www.ugr.es/`.
  * Hay que entender todos los resultados de `ab`:
    * Puerto
    * Encriptación usada para HTTPS
    * Tamaño de información devuelto
    * Tiempo que le ha llevado ejecutar todo el test
    * Número de peticiones que han fallado
    * Total de información transferida
    * Cantidad de información HTML
    * mean = media, median = mediana
    * Connection Times: COMPLETAR WIP
      * Tiempo de conexión: tiempo que le lleva al proceso al abrir el socket, cuando abrimos una conexión TCP se ejecuta un proceso
      * Tiempo de procesamiento (_processing_): desde que hacemos la conexión hasta que nos bajamos la información
      * Tiempo de espera (_waiting_): tiempo que ha tardado en llegar el primer byte, desde que se realiza la conexión hasta la recepción de éste
      * Tiempo total (_total_): tiempo que tarda en total

* `ab` es una carga a lo bruto, no simula una carga real: Nginx es lo suficientemente inteligente para no abrir muchos procesos hijo para cada petición (pues todos vienen de la misma IP).

* Para qué sirve entonces:

  * Para ver si puede aguantar muchas peticiones, para una cosa rapidilla

* No llega para algo más complicado: **JMeter**

## JMeter

Para hacer una carga realista tenemos que distribuir la carga, similar a lo que se hace en un ataque DDoS.

* Podemos hacer cargas sintéticas: simulación de comportamiento de usuarios.
* Podemos hacer cargas reales: podemos coger un fichero de log de Apache y simular la carga (de este modo podemos usar el comportamiento de un sistema ante una carga habitual).
  * También podemos hacer carga a bases de datos y a otras muchas cosas.
  * Podemos también simular, por ejemplo, "4 veces la carga del Black Friday": simulamos la carga con 4 hebras en paralelo ejecutando el log.

Vamos a lanzar la aplicación de esta práctica, `iseP4Jmeter`.

* MongoDB
  * Es _schema free_: nos deja modificar los modelos en caliente.
  * Es NoSQL y tiene optimización en lectura. Tiene varios nodos read-only y unos pocos read-write. MongoDB se encarga de mantener la coherencia.
    * Normalmente la frecuencia de lectura es mayor que la frecuencia de escritura. En este tipo de escenarios, es útil tener este tipo de BD, optimizadas para lectura.
* El frontend del servicio es el puerto 3000.
* Tenemos un puerto para acceder a Mongo, en producción no se pondría.
* Docker es el que resuelve las IP con dockercompose. Si nos fijamos en `package.json`, no explicitamos la IP en Mongo, ponemos `mongodb`.

Levantamos el servicio

~~~
docker-compose up
~~~

* Hace que los locks del sistema se queden vinculados a la consola. Si queremos lanzarlo en _background_, usamos

  ~~~
  docker-compose -d up
  ~~~

  * Para consultar los logs hay que usar `docker`.
  * Se reúnen los logs de todas las máquinas en un único stream de logs. Muy util para clusters.

* El script `pruebaentorno.sh`

  * Prueba que el endpoint sea `localhost`: sólo podemos ejecutarlo desde la propia máquina.
  * Llama a `curl`.
    * Se puede guardar el resultado con `$()`. Retorna un JWT (JSON Web Token), muy usado en interfaces tipo REST.
      * Uso de verbos protocolo HTTP para hacer operaciones CRUD.
      * Existencia de entidades en el que haremos transferencias.
      * No guardamos sesión en el servidor. ¿Cómo mantenemos la identidad del usuario?
        * Tradicionalmente mediante cookies, pero la API REST no nos deja.
        * Pasamos en su lugar tokens completos.
    * Hay que aprender a hacer GET y POST.

* Acceso proporcionado por `BasicAuth`. Las credenciales se mandan en abierto, sin cifrar.

  * Se suele usar para proteger endpoints, servicios sacados a Internet público.
  * Ruido de Internet: en cuanto abrimos un puerto a Internet público empezamos a recibir ataques.
  * Si dejamos el servicio abierto vamos a recibir peticiones con login y password, este BasicAuth hace que solo las personas que conozcan unas ciertas credenciales puedan acceder.
  * Se suele usar para APIs privadas.

* Validar JWT online: jwt.io

  * Register claims principales
    * `sub`: PK del individuo
    * `iat`: momento en el que se generó el token
    * `nof`: token no es válido antes de un momento dado (para tener que esperar un poco hasta cargar)
    * `exp`: cuándo expira el token
  * Este es personalizado, no es un register claim estándar
    * `role`: rol (esta info es la que consume el método GET para decir si tenes derechos o no para consultar un cierto expediente)
  * Esta validación se hace de forma casi automática: ha instalado un módulo de Node.
  * La información no está cifrada. Por tanto, no pasar información confidencial por aquí.
  * La información está firmada. Para hacer la firma se usa una llave simétrica configurada en el _endpoint_. La clave está en `nodejs/config/config.json`, en `jwtTokenSecret`.
  * Muchas veces el ordenador que genera el token no es el mismo que lo consume. Hay protocolos más avanzados, como OAuth, basado también en JWT.
  * JWT:
    * No está cifrado
    * Está firmado

Comenzamos a trabajar sobre JMeter.

##### Pruebas de carga con JMeter

* Hay que hacerlas en un ordenador distinto (donde corre JMeter debe ser distinto a donde corra Kubernetes). Para probar, ejecutamos `pruebaEntorno.sh` desde el ordenador donde esté JMeter.

* JMeter corre sobre Java, instalar una versión superior a 8. El profesor usará Java 11.

* Entramos en la interfaz de JMeter. 

  ~~~
  jMeter jmeter &
  ~~~

* En **Test Plan** nombramos el plan.

* Configuramos un Thread Group: botón derecho sobre plan > Add > Threads (Users) > Thread Group. Los grupos de hebras simulan la carga de trabajo. Hemos de simular dos perfiles de usuarios distintos (alumnos y administrativos).

  * Creamos un grupo para los alumnos
  * Podemos usar JMeters para tests funcionales, pero suele usarse para test de carga (_assertions_).
  * Thread Properties:
    * Number of Threads (users): número de hebras concurrentes, simula el número de usuarios que van a estar usando el sistema.
    * Ramp-up Period: segundos en los que se distribuirán. Permite que el sistema aloque recursos de forma progresiva, que es un comportamiento más usual.
    * Loop Count
    * Scheduler: planificación

* Añadimos un **Sampler**. Add > Sampler >> HTTP Request. Hay muchos Plugins. Para empezar solo pondremos la página principal.

  * Protocol: http por defecto, cambiar si usamos https.
  * Dirección IP y puerto (3000).

* Añadimos un **Listener**. Add > Listener > View Results Tree. Es para depurar, no lo usaremos cuando lancemos la carga completa.

* Vemos que como User-Agent pasa Apache-HttpClient. Mejor poner una cabecera propia de un browser, por si alguien nos lo deniega.

## Otras cuestiones

* **CineBench**: un benchmark muy chulo
  * Evalúa la capacidad de procesador para lanzar hebras en paralelo.

* Utilidad para ver JSON: `jq`
  * Uso: `... | jq`
* También hay dos interfaces similares a JMeter: Gatling y Locus, mucho más funcionales.